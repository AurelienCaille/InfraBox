{% set CLUSTER_NAME = env['deployment'] + '-' + env['name'] %}

resources:

- name: {{ properties['gkeClusterName'] }}
  type: container.v1.cluster
  properties:
    zone: {{ properties['zone'] }}
    cluster:
      name: {{ properties['gkeClusterName'] }}
      initialClusterVersion: 1.9.2-gke.1
      legacyAbac:
        enabled: false
      initialNodeCount: {{ properties['initialNodeCount'] }}
      nodeConfig:
        machineType: {{ properties["instanceType"] }}
        oauthScopes:
        - https://www.googleapis.com/auth/logging.write
        - https://www.googleapis.com/auth/monitoring

- type: runtimeconfig.v1beta1.config
  name: {{ CLUSTER_NAME }}-config
  properties:
    config: {{ CLUSTER_NAME }}-config

- type: runtimeconfig.v1beta1.waiter
  name: {{ CLUSTER_NAME }}-waiter
  metadata:
    dependsOn:
    - {{ CLUSTER_NAME }}-config
  properties:
    parent: $(ref.{{ CLUSTER_NAME }}-config.name)
    waiter: {{ CLUSTER_NAME }}-waiter
    timeout: 600s
    success:
      cardinality:
        path: /success
        number: 1
    failure:
      cardinality:
        path: /failure
        number: 1

- name: {{ CLUSTER_NAME }}-vm
  type: compute.v1.instance
  metadata:
    dependsOn:
    - {{ properties['gkeClusterName'] }}
  properties:
    zone: {{ properties['zone'] }}
    machineType: https://www.googleapis.com/compute/v1/projects/{{ env["project"] }}/zones/{{ properties["zone"] }}/machineTypes/{{ properties["instanceType"] }}
    tags:
      items:
      -  infrabox-init
    serviceAccounts:
      - email: "default"
        scopes:
        - https://www.googleapis.com/auth/cloud-platform
        - https://www.googleapis.com/auth/compute
        - https://www.googleapis.com/auth/logging.write
        - https://www.googleapis.com/auth/monitoring
        - https://www.googleapis.com/auth/servicecontrol
        - https://www.googleapis.com/auth/service.management.readonly
        - https://www.googleapis.com/auth/userinfo.email

    networkInterfaces:
    - network: https://www.googleapis.com/compute/v1/projects/{{ env["project"] }}/global/networks/default
      accessConfigs:
      - name: External NAT
        type: ONE_TO_ONE_NAT
    disks:
    - deviceName: boot
      type: PERSISTENT
      boot: true
      autoDelete: true
      initializeParams:
        diskName: {{ CLUSTER_NAME }}-vm-disk
        sourceImage: https://www.googleapis.com/compute/v1/projects/debian-cloud/global/images/debian-8-jessie-v20170918
    metadata:
      items:
      - key: startup-script
        value: |
          #!/bin/bash -x
          apt-get update && apt-get install -y git curl kubectl python python-yaml
          export HOME=/root
          gcloud components update -q
          gcloud components install beta -q
          gcloud container clusters get-credentials {{ properties['gkeClusterName'] }} --zone {{ properties['zone'] }}

          # Install helm
          curl -LO https://storage.googleapis.com/kubernetes-helm/helm-v2.7.0-linux-amd64.tar.gz
          tar xvf helm-v2.7.0-linux-amd64.tar.gz
          mv ./linux-amd64/helm /usr/bin/helm
          rm -rf linux-amd64
          rm helm-v2.7.0-linux-amd64.tar.gz

          # Init helm
          export PW=$(gcloud container clusters describe {{ properties['gkeClusterName'] }} --zone {{ properties['zone'] }} | grep password | awk '{ print $2 }')
          kubectl -n kube-system create sa tiller
          kubectl --username=admin --password=$PW create clusterrolebinding tiller --clusterrole cluster-admin --serviceaccount=kube-system:tiller
          helm init --service-account tiller
          kubectl rollout status -w deployment/tiller-deploy --namespace=kube-system

          # Create namespaces
          kubectl create ns infrabox-system
          kubectl create ns infrabox-worker

          # Install nginx ingress controller
          helm install \
            -n nginx-ingress-controller \
            --namespace kube-system \
            --set rbac.create=true \
            --set controller.service.loadBalancerIP={{ properties['externalLBIP'] }} \
            --set controller.config.proxy-body-size="0" \
            stable/nginx-ingress

          # TLS
          openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout /tmp/tls.key -out /tmp/tls.crt -subj "/CN={{ properties['domainName'] }}"
          kubectl create -n infrabox-system secret tls infrabox-tls-certs --key /tmp/tls.key --cert /tmp/tls.crt

          # Postgres
          helm install -n postgres --namespace infrabox-system --set postgresPassword=qweasdzxc1,postgresUser=infrabox,postgresDatabase=infrabox stable/postgresql

          # Minio
          helm install stable/minio --set serviceType=ClusterIP --namespace infrabox-system -n minio

          cat >/tmp/minio-init.yaml <<EOL
          apiVersion: batch/v1
          kind: Job
          metadata:
              name: init-minio
              namespace: infrabox-system
          spec:
              template:
                  metadata:
                      name: init-minio
                  spec:
                      containers:
                      -
                          name: init-minio
                          image: minio/mc
                          command: ["/bin/sh", "-c"]
                          args: ["mc config host add infrabox http://minio-minio-svc.infrabox-system:9000 AKIAIOSFODNN7EXAMPLE wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY S3v4 && mc mb infrabox/infrabox-container-output && mc mb infrabox/infrabox-project-upload && mc mb infrabox/infrabox-container-content-cache && mc mb infrabox/infrabox-docker-registry && mc ls infrabox"]
                      restartPolicy: Never
          EOL

          kubectl create -f /tmp/minio-init.yaml

          # InfraBox
          git clone https://github.com/infrabox/infrabox /tmp/infrabox
          ssh-keygen -N '' -t rsa -f id_rsa
          ssh-keygen -f id_rsa.pub -e -m pem > id_rsa.pem

          cat >install.sh <<EOL
          python /tmp/infrabox/deploy/install.py \\
            --version build_393 \\
            -o /tmp/infrabox-configuration \\
            --platform kubernetes \\
            --general-rsa-public-key ./id_rsa.pem \\
            --general-rsa-private-key ./id_rsa \\
            --root-url https://{{ properties['domainName'] }} \\
            --database postgres \\
            --postgres-host postgres-postgresql.infrabox-system \\
            --postgres-username infrabox \\
            --postgres-database infrabox \\
            --postgres-port 5432 \\
            --postgres-password qweasdzxc1 \\
            --storage s3 \\
            --s3-access-key AKIAIOSFODNN7EXAMPLE \\
            --s3-secret-key wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY \\
            --s3-secure false \\
            --s3-endpoint minio-minio-svc.infrabox-system \\
            --s3-port 9000 \\
            --s3-region us-east-1 \\
            {% if properties['githubEnabled'] %}
            --github-enabled \\
            --github-client-id {{ properties['githubClientID'] }} \\
            --github-client-secret {{ properties['githubClientSecret'] }} \\
            --github-webhook-secret asdfj340rcdf09347caefaasfd897 \\
            --github-login-enabled \\
            {% if properties['githubLoginAllowedOrganizationsEnabled'] %}
            --github-login-allowed-organizations {{ properties['githubLoginAllowedOrganizations'] }} \\
            {% endif %}
            {% else %}
            --account-signup-enabled \\
            {% endif %}
            --docker-registry-admin-username admin \\
            --docker-registry-admin-password {{ properties['dockerRegistryPassword'] }}
          EOL
          chmod +x install.sh
          sed -i '/^\s*$/d' install.sh
          cat install.sh

          ./install.sh

          cd /tmp/infrabox-configuration/infrabox
          helm install -n infrabox .

          # Done
          gcloud beta runtime-config configs variables set success/{{ CLUSTER_NAME }}-waiter success --config-name $(ref.{{ CLUSTER_NAME }}-config.name)
          # gcloud -q compute instances delete {{ CLUSTER_NAME }}-vm --zone {{ properties['zone'] }}
